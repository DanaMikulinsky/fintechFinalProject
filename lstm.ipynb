{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-07T17:18:32.478193Z",
     "start_time": "2024-09-07T17:18:28.339949Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:20:54.474799Z",
     "start_time": "2024-09-07T17:20:54.399190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_data_df = pd.read_csv('all_data.csv')\n",
    "X = all_data_df.drop(columns=['gold_price_to_predict', 'Date'])\n",
    "\n",
    "# Extract y\n",
    "y = all_data_df['gold_price_to_predict']\n",
    "X_dropped = X.dropna().astype(np.float32)\n",
    "y_dropped = y[X_dropped.index].to_numpy()[:-1].astype(np.float32)\n",
    "X_dropped = X_dropped.to_numpy()[:-1]\n",
    "\n",
    "def create_sequences(data, y, seq_length):\n",
    "    sequences = []\n",
    "    y_fit = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])  # Last 30 days as input\n",
    "        y_fit.append(y[i])\n",
    "    return np.array(sequences), np.array(y_fit)\n",
    "\n",
    "\n",
    "sequence_length = 4\n",
    "X_seqs, y_fit = create_sequences(X_dropped, y, sequence_length)\n",
    "X_seqs = torch.tensor(X_seqs, dtype=torch.float32)\n",
    "y_fit = torch.tensor(y_fit, dtype=torch.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seqs, y_fit, test_size=0.2, shuffle=False)\n"
   ],
   "id": "c8ed9c084cba1e96",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:20:55.956287Z",
     "start_time": "2024-09-07T17:20:55.952019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n"
   ],
   "id": "ffab1bb649a32305",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:20:56.702979Z",
     "start_time": "2024-09-07T17:20:56.693234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader with batching and shuffling\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "93eb398051dbbf46",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:20:57.494956Z",
     "start_time": "2024-09-07T17:20:57.476893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq: shape (batch_size, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(input_seq)  # No need to reshape\n",
    "        predictions = self.linear(lstm_out[:, -1, :])  # Only take the output of the last time step\n",
    "        return predictions\n"
   ],
   "id": "8cb8d9a2ed2ab5c3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:21:01.300561Z",
     "start_time": "2024-09-07T17:20:58.471584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMModel(input_size=X_train.shape[2], hidden_layer_size=100, output_size=1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "1169cf8a426b5331",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:21:47.630998Z",
     "start_time": "2024-09-07T17:21:22.195718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 150\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:  # Iterate over batches\n",
    "        model.hidden_cell = (torch.zeros(1, X_batch.size(0), model.hidden_layer_size),\n",
    "                             torch.zeros(1, X_batch.size(0), model.hidden_layer_size))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(X_batch)\n",
    "        loss = loss_function(torch.ravel(predictions), y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*y_batch.shape[0]\n",
    "    print(f'Epoch {epoch} Loss: {loss.item()/len(train_dataset)}')"
   ],
   "id": "1dbb6cf638ccea7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 8.585664216222124\n",
      "Epoch 1 Loss: 7.9778939044053345\n",
      "Epoch 2 Loss: 7.891674580509401\n",
      "Epoch 3 Loss: 8.119593251257106\n",
      "Epoch 4 Loss: 7.760542775743332\n",
      "Epoch 5 Loss: 7.415478451574114\n",
      "Epoch 6 Loss: 6.758885992839965\n",
      "Epoch 7 Loss: 7.557126557717534\n",
      "Epoch 8 Loss: 7.261021876366419\n",
      "Epoch 9 Loss: 6.806498432034324\n",
      "Epoch 10 Loss: 6.551823998414954\n",
      "Epoch 11 Loss: 6.7898059172770004\n",
      "Epoch 12 Loss: 7.073076509209663\n",
      "Epoch 13 Loss: 6.824816558264102\n",
      "Epoch 14 Loss: 7.042503313565807\n",
      "Epoch 15 Loss: 6.158320978629209\n",
      "Epoch 16 Loss: 6.5452126660198955\n",
      "Epoch 17 Loss: 5.919405624863358\n",
      "Epoch 18 Loss: 6.427821057198295\n",
      "Epoch 19 Loss: 6.205158402109751\n",
      "Epoch 20 Loss: 6.083119688046567\n",
      "Epoch 21 Loss: 5.785123370545475\n",
      "Epoch 22 Loss: 6.373032356799301\n",
      "Epoch 23 Loss: 5.842989929492785\n",
      "Epoch 24 Loss: 5.845882040473327\n",
      "Epoch 25 Loss: 5.500927883827066\n",
      "Epoch 26 Loss: 5.755364475158505\n",
      "Epoch 27 Loss: 5.454174580509401\n",
      "Epoch 28 Loss: 5.551536623442282\n",
      "Epoch 29 Loss: 5.440575723518802\n",
      "Epoch 30 Loss: 5.2674517312527325\n",
      "Epoch 31 Loss: 4.911058940478793\n",
      "Epoch 32 Loss: 4.858287415965238\n",
      "Epoch 33 Loss: 4.2537734511641885\n",
      "Epoch 34 Loss: 4.742991979121119\n",
      "Epoch 35 Loss: 4.664898577557937\n",
      "Epoch 36 Loss: 5.0581513718845645\n",
      "Epoch 37 Loss: 4.58983350185833\n",
      "Epoch 38 Loss: 4.356390655744425\n",
      "Epoch 39 Loss: 4.406331985133362\n",
      "Epoch 40 Loss: 4.444275302661784\n",
      "Epoch 41 Loss: 4.555455256613468\n",
      "Epoch 42 Loss: 3.8775227508745083\n",
      "Epoch 43 Loss: 4.398979370490817\n",
      "Epoch 44 Loss: 4.061297978383253\n",
      "Epoch 45 Loss: 4.304170822857455\n",
      "Epoch 46 Loss: 4.03125298904132\n",
      "Epoch 47 Loss: 3.678973119124399\n",
      "Epoch 48 Loss: 3.7476903250710536\n",
      "Epoch 49 Loss: 3.985109450153039\n",
      "Epoch 50 Loss: 3.2509603362756887\n",
      "Epoch 51 Loss: 3.65478665077066\n",
      "Epoch 52 Loss: 3.029946350978356\n",
      "Epoch 53 Loss: 3.422863091659379\n",
      "Epoch 54 Loss: 3.188732766041758\n",
      "Epoch 55 Loss: 3.0207127753334064\n",
      "Epoch 56 Loss: 3.065524696313402\n",
      "Epoch 57 Loss: 2.778484282766725\n",
      "Epoch 58 Loss: 2.9917160854831657\n",
      "Epoch 59 Loss: 2.93378761067993\n",
      "Epoch 60 Loss: 2.643867042317993\n",
      "Epoch 61 Loss: 2.7544632792003716\n",
      "Epoch 62 Loss: 3.1699293476033015\n",
      "Epoch 63 Loss: 3.1443388838407302\n",
      "Epoch 64 Loss: 3.2515995641123743\n",
      "Epoch 65 Loss: 2.7384023061734806\n",
      "Epoch 66 Loss: 2.757765102344775\n",
      "Epoch 67 Loss: 2.54813722944906\n",
      "Epoch 68 Loss: 2.822555989013992\n",
      "Epoch 69 Loss: 2.4283129679984694\n",
      "Epoch 70 Loss: 1.9160941940861391\n",
      "Epoch 71 Loss: 2.5196224157602756\n",
      "Epoch 72 Loss: 2.1486283716386096\n",
      "Epoch 73 Loss: 2.286057445958133\n",
      "Epoch 74 Loss: 2.494515109176869\n",
      "Epoch 75 Loss: 2.182141289421185\n",
      "Epoch 76 Loss: 1.5165900333406208\n",
      "Epoch 77 Loss: 2.1223884318976824\n",
      "Epoch 78 Loss: 2.064669830495737\n",
      "Epoch 79 Loss: 2.2550275675010933\n",
      "Epoch 80 Loss: 2.0213449575726936\n",
      "Epoch 81 Loss: 1.844149677525142\n",
      "Epoch 82 Loss: 2.0558265382460648\n",
      "Epoch 83 Loss: 1.5819970895277655\n",
      "Epoch 84 Loss: 1.9901109702940534\n",
      "Epoch 85 Loss: 1.8483909136559904\n",
      "Epoch 86 Loss: 1.6499352232045257\n",
      "Epoch 87 Loss: 1.7054174879413533\n",
      "Epoch 88 Loss: 1.6984676466167468\n",
      "Epoch 89 Loss: 1.74302048176514\n",
      "Epoch 90 Loss: 1.4374779024445234\n",
      "Epoch 91 Loss: 1.4861287132433318\n",
      "Epoch 92 Loss: 1.471930233213544\n",
      "Epoch 93 Loss: 1.5172545612770552\n",
      "Epoch 94 Loss: 1.2363791522054002\n",
      "Epoch 95 Loss: 1.771162092294764\n",
      "Epoch 96 Loss: 1.2439226384865545\n",
      "Epoch 97 Loss: 1.3044067436188238\n",
      "Epoch 98 Loss: 1.086408594262407\n",
      "Epoch 99 Loss: 1.3099468249549082\n",
      "Epoch 100 Loss: 1.1655761291744098\n",
      "Epoch 101 Loss: 1.2972177789885768\n",
      "Epoch 102 Loss: 1.337486036906974\n",
      "Epoch 103 Loss: 0.9260448193935833\n",
      "Epoch 104 Loss: 1.1887164330659707\n",
      "Epoch 105 Loss: 1.2116294630656974\n",
      "Epoch 106 Loss: 1.0011517416716769\n",
      "Epoch 107 Loss: 1.1712745229490051\n",
      "Epoch 108 Loss: 0.6663344382822748\n",
      "Epoch 109 Loss: 0.8176968219658669\n",
      "Epoch 110 Loss: 0.955816524957641\n",
      "Epoch 111 Loss: 0.7395495109074388\n",
      "Epoch 112 Loss: 1.0761850054315152\n",
      "Epoch 113 Loss: 1.0075066570220266\n",
      "Epoch 114 Loss: 0.9270861800393528\n",
      "Epoch 115 Loss: 0.8361105441421622\n",
      "Epoch 116 Loss: 0.9670273514361063\n",
      "Epoch 117 Loss: 0.911575617621338\n",
      "Epoch 118 Loss: 0.7699583626544053\n",
      "Epoch 119 Loss: 0.7744418712594283\n",
      "Epoch 120 Loss: 0.5235770241787823\n",
      "Epoch 121 Loss: 0.7929084354162111\n",
      "Epoch 122 Loss: 0.623773692422524\n",
      "Epoch 123 Loss: 0.8027312792072038\n",
      "Epoch 124 Loss: 0.5611563191749562\n",
      "Epoch 125 Loss: 0.6776676019519294\n",
      "Epoch 126 Loss: 0.5743750341604722\n",
      "Epoch 127 Loss: 0.5085201021568923\n",
      "Epoch 128 Loss: 0.6311187277102919\n",
      "Epoch 129 Loss: 0.5248238280395988\n",
      "Epoch 130 Loss: 0.63309016058838\n",
      "Epoch 131 Loss: 0.47506312214760055\n",
      "Epoch 132 Loss: 0.5257051148475077\n",
      "Epoch 133 Loss: 0.574989068648885\n",
      "Epoch 134 Loss: 0.38998539853314934\n",
      "Epoch 135 Loss: 0.7802937736561271\n",
      "Epoch 136 Loss: 0.47374490154951904\n",
      "Epoch 137 Loss: 0.416222269169149\n",
      "Epoch 138 Loss: 0.33621089052935066\n",
      "Epoch 139 Loss: 0.230612357422729\n",
      "Epoch 140 Loss: 0.213907085864055\n",
      "Epoch 141 Loss: 0.5002835319195452\n",
      "Epoch 142 Loss: 0.46404338081069635\n",
      "Epoch 143 Loss: 0.5877849944147628\n",
      "Epoch 144 Loss: 0.5346955640064768\n",
      "Epoch 145 Loss: 0.3921705745535226\n",
      "Epoch 146 Loss: 0.5385438479551541\n",
      "Epoch 147 Loss: 0.4828257158326957\n",
      "Epoch 148 Loss: 0.3649922893409078\n",
      "Epoch 149 Loss: 0.4894182599680258\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T17:22:09.690551Z",
     "start_time": "2024-09-07T17:22:09.664998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    future_price = torch.ravel(model(X_test))\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, future_price))\n",
    "    mae = mean_absolute_error(y_test, future_price)\n",
    "\n",
    "    print(f\"Root Mean Square Error: {rmse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")"
   ],
   "id": "9fc6287f1d3a5cec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error: 68.819580078125\n",
      "Mean Absolute Error: 66.1340560913086\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab77e108b9490309"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
